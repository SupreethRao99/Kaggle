{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "DeepCAPTCHA.ipynb",
      "provenance": [],
      "mount_file_id": "1oahmAcbdXZS0wB0bV6MGXCzGLY97YY68",
      "authorship_tag": "ABX9TyMuWTVn76Zgkupa5/VK2kR0",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SupreethRao99/Kaggle/blob/master/DeepCAPTCHA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndHc1SnuprQ3"
      },
      "source": [
        "# DeepCAPTCHA\n",
        "DeepCAPTHA is a ResNet architecture based convultional neural network (CNN) trained on the [Chars74K-Fonts](http://www.ee.surrey.ac.uk/CVSSP/demos/chars74k/#download) Dataset. It has been built as part of a larger project which attempts to defeat simple CAPTCHAs. The Network trained in this notebook achieves an training accuracy of 88% and a validation accuracy of 87%."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdqEFYv93qM9"
      },
      "source": [
        "# importing the required libraries\n",
        "from google.colab import drive\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import os\n",
        "import random\n",
        "from shutil import copyfile\n",
        "import datetime"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9c8oiJKSqVm",
        "outputId": "16d15273-6664-47e5-bd3e-f40cce6e7253"
      },
      "source": [
        "!nvidia-smi #displays the GPU allocated by google colab"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Apr  4 16:36:55 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.67       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P0    25W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vL1T5sO2rAiF"
      },
      "source": [
        "The dataset is stored on google drive. The dataset is then loaded onto colab and unzipped. training and testing directories are created for each class present in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XvTsFgYPCQBg",
        "outputId": "5681bf14-61b2-48c6-972f-a4d6e2379f9e"
      },
      "source": [
        "drive.mount('/content/drive') "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZxhm3R3BuPZ"
      },
      "source": [
        "zip_ref = zipfile.ZipFile(\"/content/drive/MyDrive/English.zip\", 'r')\n",
        "zip_ref.extractall(\"/tmp\")\n",
        "zip_ref.close()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5Sies0tLdjh"
      },
      "source": [
        "os.mkdir('/tmp/CAPTCHA')\n",
        "os.mkdir('/tmp/CAPTCHA/testing')\n",
        "os.mkdir('/tmp/CAPTCHA/training')\n",
        "for i in range(0,62):\n",
        "  try:\n",
        "    if i>= 0 and i<10:\n",
        "      os.mkdir('/tmp/CAPTCHA/training/'+chr(i+48))\n",
        "      os.mkdir('/tmp/CAPTCHA/testing/'+chr(i+48))\n",
        "    if i>= 10 and i<36:\n",
        "      os.mkdir('/tmp/CAPTCHA/training/'+chr(i-10+65))\n",
        "      os.mkdir('/tmp/CAPTCHA/testing/'+chr(i-10+65))\n",
        "    if i>=36 and i<62:\n",
        "      os.mkdir('/tmp/CAPTCHA/training/'+chr(i-36+97))\n",
        "      os.mkdir('/tmp/CAPTCHA/testing/'+chr(i-36+97))\n",
        "      \n",
        "  except OSError:\n",
        "    print('failed')\n",
        "    pass"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTBt5cd2raSd"
      },
      "source": [
        "the `split_data` function splits the dataset into training and testing sets randomly. the size of the training and testing set is determined by the `SPLIT_SIZE` parameter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWjftMSCKJtj"
      },
      "source": [
        "from random import shuffle\n",
        "import shutil\n",
        "\n",
        "def split_data(SOURCE, TRAINING, TESTING, SPLIT_SIZE):\n",
        "    all_images = os.listdir(SOURCE)\n",
        "    shuffle(all_images)\n",
        "    splitting_index = round(SPLIT_SIZE*len(all_images))\n",
        "    train_images = all_images[:splitting_index]\n",
        "    test_images = all_images[splitting_index:]\n",
        "\n",
        "    for img in train_images:\n",
        "        src = os.path.join(SOURCE, img)\n",
        "        dst = os.path.join(TRAINING, img)\n",
        "        if os.path.getsize(src) <= 0:\n",
        "            print(img+\" is zero length, so ignoring!!\")\n",
        "        else:\n",
        "            shutil.copyfile(src, dst)\n",
        "\n",
        "    for img in test_images:\n",
        "        src = os.path.join(SOURCE, img)\n",
        "        dst = os.path.join(TESTING, img)\n",
        "        if os.path.getsize(src) <= 0:\n",
        "            print(img+\" is zero length, so ignoring!!\")\n",
        "        else:\n",
        "            shutil.copyfile(src, dst)\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9H_hfeTKsI8"
      },
      "source": [
        "split_size = 0.90\n",
        "for i in range(0,62):\n",
        "  if i>=0 and i<10:\n",
        "    split_data('/tmp/English/Fnt/'+chr(i+48),\n",
        "               '/tmp/CAPTCHA/training/'+chr(i+48),\n",
        "               '/tmp/CAPTCHA/testing/'+chr(i+48),\n",
        "               split_size)\n",
        "  if i>=10 and i<36:\n",
        "    split_data('/tmp/English/Fnt/'+chr(i-10+65)+\"-1\",\n",
        "               '/tmp/CAPTCHA/training/'+chr(i-10+65),\n",
        "               '/tmp/CAPTCHA/testing/'+chr(i-10+65),\n",
        "               split_size)\n",
        "  if i>=36 and i<62:\n",
        "    split_data('/tmp/English/Fnt/'+chr(i-36+97)+\"-2\",\n",
        "               '/tmp/CAPTCHA/training/'+chr(i-36+97),\n",
        "               '/tmp/CAPTCHA/testing/'+chr(i-36+97),\n",
        "               split_size)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSvUg3QzrClp"
      },
      "source": [
        "# Creation of Model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwqnXOF_bWfe"
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rA1eAnP3sVrM"
      },
      "source": [
        "Images are augmented by rescaling, rotating , shearing, zooming and flipping. This provides a cheap and very effective way to provide more data for the model to learn from."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQMC1ai1u9sm",
        "outputId": "e09f76d4-9cf5-48ec-e59a-a0c813239fb0"
      },
      "source": [
        "TRAINING_DIR = '/tmp/CAPTCHA/training'\n",
        "train_datagen = ImageDataGenerator(rescale=1. / 255,\n",
        "                                   rotation_range=30,\n",
        "                                   shear_range=0.2,\n",
        "                                   zoom_range=0.2,\n",
        "                                   horizontal_flip=True,\n",
        "                                   fill_mode='nearest',\n",
        "            preprocessing_function = tf.image.rgb_to_grayscale)\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    TRAINING_DIR,\n",
        "    target_size = (32,32),\n",
        "    batch_size = 16,\n",
        "    class_mode = 'categorical'\n",
        ")\n",
        "\n",
        "VALIDATION_DIR = '/tmp/CAPTCHA/testing'\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255,\n",
        "            preprocessing_function = tf.image.rgb_to_grayscale)\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    VALIDATION_DIR,\n",
        "    target_size = (32,32),\n",
        "    batch_size = 16,\n",
        "    class_mode = 'categorical'\n",
        ")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 56668 images belonging to 62 classes.\n",
            "Found 6324 images belonging to 62 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3n_Ac88ls0bX"
      },
      "source": [
        "## The ResNet Model\n",
        "the model described below uses a custom model based on the [ResNet architecture](https://arxiv.org/pdf/1512.03385.pdf) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEHqNmrg588-"
      },
      "source": [
        "import keras\n",
        "from functools import partial\n",
        "DefaultConv2D = partial(keras.layers.Conv2D, kernel_size=3, strides=1,\n",
        "                        padding=\"SAME\", use_bias=False)\n",
        "\n",
        "class ResidualUnit(keras.layers.Layer):\n",
        "    def __init__(self, filters, strides=1, activation=\"relu\", **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.activation = keras.activations.get(activation)\n",
        "        self.main_layers = [\n",
        "            DefaultConv2D(filters, strides=strides),\n",
        "            keras.layers.BatchNormalization(),\n",
        "            self.activation,\n",
        "            DefaultConv2D(filters),\n",
        "            keras.layers.BatchNormalization()]\n",
        "        self.skip_layers = []\n",
        "        if strides > 1:\n",
        "            self.skip_layers = [\n",
        "                DefaultConv2D(filters, kernel_size=1, strides=strides),\n",
        "                keras.layers.BatchNormalization()]\n",
        "\n",
        "    def get_config(self):\n",
        "      cfg = super().get_config()\n",
        "      return cfg  \n",
        "\n",
        "    def call(self, inputs):\n",
        "        Z = inputs\n",
        "        for layer in self.main_layers:\n",
        "            Z = layer(Z)\n",
        "        skip_Z = inputs\n",
        "        for layer in self.skip_layers:\n",
        "            skip_Z = layer(skip_Z)\n",
        "        return self.activation(Z + skip_Z)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aX2FYSbN8Uf7",
        "outputId": "4d861676-0750-47f1-f56f-fc5808493d7c"
      },
      "source": [
        "keras.backend.clear_session()\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "model = keras.models.Sequential()\n",
        "model.add(DefaultConv2D(64, kernel_size=4, strides=2,\n",
        "                        input_shape=[32, 32, 3]))\n",
        "model.add(keras.layers.BatchNormalization())\n",
        "model.add(keras.layers.Activation(\"relu\"))\n",
        "model.add(keras.layers.MaxPool2D(pool_size=3, strides=2, padding=\"SAME\"))\n",
        "prev_filters = 64\n",
        "for filters in [64] * 2 + [128] * 2 + [256] * 1 :\n",
        "    strides = 1 if filters == prev_filters else 2\n",
        "    model.add(ResidualUnit(filters, strides=strides))\n",
        "    prev_filters = filters\n",
        "model.add(keras.layers.GlobalAvgPool2D())\n",
        "model.add(keras.layers.Flatten())\n",
        "model.add(keras.layers.Dropout(0.5))\n",
        "model.add(keras.layers.Dense(62, activation=\"softmax\"))\n",
        "\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"nadam\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 16, 16, 64)        3072      \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 16, 16, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "residual_unit (ResidualUnit) (None, 8, 8, 64)          74240     \n",
            "_________________________________________________________________\n",
            "residual_unit_1 (ResidualUni (None, 8, 8, 64)          74240     \n",
            "_________________________________________________________________\n",
            "residual_unit_2 (ResidualUni (None, 4, 4, 128)         230912    \n",
            "_________________________________________________________________\n",
            "residual_unit_3 (ResidualUni (None, 4, 4, 128)         295936    \n",
            "_________________________________________________________________\n",
            "residual_unit_4 (ResidualUni (None, 2, 2, 256)         920576    \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d (Gl (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 62)                15934     \n",
            "=================================================================\n",
            "Total params: 1,615,166\n",
            "Trainable params: 1,611,710\n",
            "Non-trainable params: 3,456\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIG7HCQ6AQ38"
      },
      "source": [
        "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
        "\n",
        "# performance scheduling is used to reduce learning rate\n",
        "# to improve model training accuracy\n",
        "\n",
        "learning_rate_reduction = tf.keras.callbacks.ReduceLROnPlateau(\n",
        "    monitor='val_accuracy', factor=0.5, patience=2,min_lr=0.00001,mode='auto')"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ngXcpo7m_iMg",
        "outputId": "d5bbeb6c-eeea-4d72-d29a-a78146a23a1e"
      },
      "source": [
        "history = model.fit(train_generator, epochs = 50,\n",
        "                    validation_data=validation_generator,\n",
        "          callbacks=[tensorboard_callback, learning_rate_reduction])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "3542/3542 [==============================] - 205s 48ms/step - loss: 2.0175 - accuracy: 0.4438 - val_loss: 0.7236 - val_accuracy: 0.7524\n",
            "Epoch 2/50\n",
            "3542/3542 [==============================] - 170s 48ms/step - loss: 0.8330 - accuracy: 0.7255 - val_loss: 0.6567 - val_accuracy: 0.7702\n",
            "Epoch 3/50\n",
            "3542/3542 [==============================] - 170s 48ms/step - loss: 0.7066 - accuracy: 0.7636 - val_loss: 0.5393 - val_accuracy: 0.8076\n",
            "Epoch 4/50\n",
            "3542/3542 [==============================] - 171s 48ms/step - loss: 0.6394 - accuracy: 0.7770 - val_loss: 0.5376 - val_accuracy: 0.7962\n",
            "Epoch 5/50\n",
            "3542/3542 [==============================] - 170s 48ms/step - loss: 0.5793 - accuracy: 0.7930 - val_loss: 0.4853 - val_accuracy: 0.8178\n",
            "Epoch 6/50\n",
            "3542/3542 [==============================] - 170s 48ms/step - loss: 0.5482 - accuracy: 0.8018 - val_loss: 0.4812 - val_accuracy: 0.8175\n",
            "Epoch 7/50\n",
            "3542/3542 [==============================] - 170s 48ms/step - loss: 0.5300 - accuracy: 0.8060 - val_loss: 0.4988 - val_accuracy: 0.8109\n",
            "Epoch 8/50\n",
            "3542/3542 [==============================] - 170s 48ms/step - loss: 0.4565 - accuracy: 0.8276 - val_loss: 0.3958 - val_accuracy: 0.8523\n",
            "Epoch 9/50\n",
            "3542/3542 [==============================] - 168s 48ms/step - loss: 0.4225 - accuracy: 0.8394 - val_loss: 0.3849 - val_accuracy: 0.8485\n",
            "Epoch 10/50\n",
            "3542/3542 [==============================] - 169s 48ms/step - loss: 0.4084 - accuracy: 0.8410 - val_loss: 0.3759 - val_accuracy: 0.8534\n",
            "Epoch 11/50\n",
            "3542/3542 [==============================] - 169s 48ms/step - loss: 0.3955 - accuracy: 0.8447 - val_loss: 0.3960 - val_accuracy: 0.8444\n",
            "Epoch 12/50\n",
            "3542/3542 [==============================] - 169s 48ms/step - loss: 0.3925 - accuracy: 0.8466 - val_loss: 0.3688 - val_accuracy: 0.8510\n",
            "Epoch 13/50\n",
            "3542/3542 [==============================] - 168s 47ms/step - loss: 0.3624 - accuracy: 0.8569 - val_loss: 0.3400 - val_accuracy: 0.8618\n",
            "Epoch 14/50\n",
            "3542/3542 [==============================] - 167s 47ms/step - loss: 0.3448 - accuracy: 0.8620 - val_loss: 0.3339 - val_accuracy: 0.8612\n",
            "Epoch 15/50\n",
            "3542/3542 [==============================] - 169s 48ms/step - loss: 0.3335 - accuracy: 0.8653 - val_loss: 0.3361 - val_accuracy: 0.8605\n",
            "Epoch 16/50\n",
            "3542/3542 [==============================] - 168s 48ms/step - loss: 0.3261 - accuracy: 0.8669 - val_loss: 0.3214 - val_accuracy: 0.8692\n",
            "Epoch 17/50\n",
            "3542/3542 [==============================] - 168s 47ms/step - loss: 0.3210 - accuracy: 0.8686 - val_loss: 0.3178 - val_accuracy: 0.8681\n",
            "Epoch 18/50\n",
            "3542/3542 [==============================] - 166s 47ms/step - loss: 0.3115 - accuracy: 0.8735 - val_loss: 0.3175 - val_accuracy: 0.8743\n",
            "Epoch 19/50\n",
            "3542/3542 [==============================] - 166s 47ms/step - loss: 0.3061 - accuracy: 0.8723 - val_loss: 0.3083 - val_accuracy: 0.8744\n",
            "Epoch 20/50\n",
            "3542/3542 [==============================] - 166s 47ms/step - loss: 0.3061 - accuracy: 0.8737 - val_loss: 0.3247 - val_accuracy: 0.8692\n",
            "Epoch 21/50\n",
            "3542/3542 [==============================] - 167s 47ms/step - loss: 0.3035 - accuracy: 0.8738 - val_loss: 0.3166 - val_accuracy: 0.8714\n",
            "Epoch 22/50\n",
            "3542/3542 [==============================] - 166s 47ms/step - loss: 0.2929 - accuracy: 0.8787 - val_loss: 0.3111 - val_accuracy: 0.8699\n",
            "Epoch 23/50\n",
            "3542/3542 [==============================] - 165s 47ms/step - loss: 0.2956 - accuracy: 0.8762 - val_loss: 0.3086 - val_accuracy: 0.8718\n",
            "Epoch 24/50\n",
            "3542/3542 [==============================] - 166s 47ms/step - loss: 0.2896 - accuracy: 0.8790 - val_loss: 0.3079 - val_accuracy: 0.8732\n",
            "Epoch 25/50\n",
            "3542/3542 [==============================] - 166s 47ms/step - loss: 0.2846 - accuracy: 0.8816 - val_loss: 0.3058 - val_accuracy: 0.8725\n",
            "Epoch 26/50\n",
            "3542/3542 [==============================] - 166s 47ms/step - loss: 0.2858 - accuracy: 0.8799 - val_loss: 0.3043 - val_accuracy: 0.8710\n",
            "Epoch 27/50\n",
            "3542/3542 [==============================] - 164s 46ms/step - loss: 0.2851 - accuracy: 0.8814 - val_loss: 0.3044 - val_accuracy: 0.8716\n",
            "Epoch 28/50\n",
            "3542/3542 [==============================] - 167s 47ms/step - loss: 0.2862 - accuracy: 0.8804 - val_loss: 0.3021 - val_accuracy: 0.8724\n",
            "Epoch 29/50\n",
            "3542/3542 [==============================] - 167s 47ms/step - loss: 0.2738 - accuracy: 0.8846 - val_loss: 0.3016 - val_accuracy: 0.8718\n",
            "Epoch 30/50\n",
            "3542/3542 [==============================] - 167s 47ms/step - loss: 0.2798 - accuracy: 0.8821 - val_loss: 0.3031 - val_accuracy: 0.8721\n",
            "Epoch 31/50\n",
            "3542/3542 [==============================] - 167s 47ms/step - loss: 0.2811 - accuracy: 0.8807 - val_loss: 0.3004 - val_accuracy: 0.8708\n",
            "Epoch 32/50\n",
            "3542/3542 [==============================] - 166s 47ms/step - loss: 0.2752 - accuracy: 0.8841 - val_loss: 0.3056 - val_accuracy: 0.8692\n",
            "Epoch 33/50\n",
            "3542/3542 [==============================] - 166s 47ms/step - loss: 0.2766 - accuracy: 0.8816 - val_loss: 0.3043 - val_accuracy: 0.8710\n",
            "Epoch 34/50\n",
            "3542/3542 [==============================] - 166s 47ms/step - loss: 0.2804 - accuracy: 0.8803 - val_loss: 0.3021 - val_accuracy: 0.8710\n",
            "Epoch 35/50\n",
            "3542/3542 [==============================] - 167s 47ms/step - loss: 0.2747 - accuracy: 0.8821 - val_loss: 0.3018 - val_accuracy: 0.8724\n",
            "Epoch 36/50\n",
            "3542/3542 [==============================] - 166s 47ms/step - loss: 0.2751 - accuracy: 0.8839 - val_loss: 0.3035 - val_accuracy: 0.8710\n",
            "Epoch 37/50\n",
            "3542/3542 [==============================] - 165s 47ms/step - loss: 0.2747 - accuracy: 0.8843 - val_loss: 0.3025 - val_accuracy: 0.8708\n",
            "Epoch 38/50\n",
            "3542/3542 [==============================] - 164s 46ms/step - loss: 0.2766 - accuracy: 0.8820 - val_loss: 0.2996 - val_accuracy: 0.8741\n",
            "Epoch 39/50\n",
            "3542/3542 [==============================] - 165s 47ms/step - loss: 0.2727 - accuracy: 0.8841 - val_loss: 0.2997 - val_accuracy: 0.8733\n",
            "Epoch 40/50\n",
            "3542/3542 [==============================] - 167s 47ms/step - loss: 0.2744 - accuracy: 0.8851 - val_loss: 0.3009 - val_accuracy: 0.8708\n",
            "Epoch 41/50\n",
            "3542/3542 [==============================] - 166s 47ms/step - loss: 0.2733 - accuracy: 0.8827 - val_loss: 0.2999 - val_accuracy: 0.8724\n",
            "Epoch 42/50\n",
            "3542/3542 [==============================] - 167s 47ms/step - loss: 0.2730 - accuracy: 0.8826 - val_loss: 0.3015 - val_accuracy: 0.8721\n",
            "Epoch 43/50\n",
            "3542/3542 [==============================] - 167s 47ms/step - loss: 0.2753 - accuracy: 0.8839 - val_loss: 0.2980 - val_accuracy: 0.8752\n",
            "Epoch 44/50\n",
            "3542/3542 [==============================] - 165s 47ms/step - loss: 0.2787 - accuracy: 0.8807 - val_loss: 0.2987 - val_accuracy: 0.8746\n",
            "Epoch 45/50\n",
            "3542/3542 [==============================] - 163s 46ms/step - loss: 0.2719 - accuracy: 0.8861 - val_loss: 0.2987 - val_accuracy: 0.8730\n",
            "Epoch 46/50\n",
            "3542/3542 [==============================] - 162s 46ms/step - loss: 0.2770 - accuracy: 0.8832 - val_loss: 0.3004 - val_accuracy: 0.8738\n",
            "Epoch 47/50\n",
            "3542/3542 [==============================] - 165s 47ms/step - loss: 0.2756 - accuracy: 0.8810 - val_loss: 0.2979 - val_accuracy: 0.8756\n",
            "Epoch 48/50\n",
            "3542/3542 [==============================] - 167s 47ms/step - loss: 0.2715 - accuracy: 0.8841 - val_loss: 0.2979 - val_accuracy: 0.8752\n",
            "Epoch 49/50\n",
            "3542/3542 [==============================] - 167s 47ms/step - loss: 0.2719 - accuracy: 0.8832 - val_loss: 0.2986 - val_accuracy: 0.8740\n",
            "Epoch 50/50\n",
            "3542/3542 [==============================] - 166s 47ms/step - loss: 0.2749 - accuracy: 0.8819 - val_loss: 0.2965 - val_accuracy: 0.8752\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PDSPJePuidl"
      },
      "source": [
        "The model is saved in the `saved_model` format and downloaded for use as part of the larger project."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ughhHjrLg26E"
      },
      "source": [
        "model.save('CAPTCHA-Model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byXVltIymdV8"
      },
      "source": [
        "!zip -r /content/CAPTCHA.zip /content/CAPTCHA-Model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "ZFAedABbm8VD",
        "outputId": "dddccfd8-25c9-4b64-8deb-851f128c57fd"
      },
      "source": [
        "from google.colab import files\n",
        "files.download(\"/content/CAPTCHA.zip\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_f298a253-e9c2-4b9a-a839-2648f107bf23\", \"CAPTCHA.zip\", 17857691)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}